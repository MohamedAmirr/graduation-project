{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoKta9LCHDZR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the GPU\n",
        "!nvidia-smi\n",
        "\n"
      ],
      "metadata": {
        "id": "RS19899lHH5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies.\n",
        "!pip install bitsandbytes transformers accelerate peft -q\n",
        "!pip install git+https://github.com/huggingface/diffusers.git -q\n",
        "!wget https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora_sdxl.py\n",
        "!pip install datasets -q\n"
      ],
      "metadata": {
        "id": "D7iTwZm4HMjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "import gc\n",
        "import locale\n",
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import upload_folder\n",
        "from IPython.display import display, Markdown\n",
        "from diffusers import DiffusionPipeline, AutoencoderKL"
      ],
      "metadata": {
        "id": "U1UK_iA_HWMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder_path):\n",
        "    img_paths = []\n",
        "    for ext in ['*.png', '*.jpg', '*.jpeg', '*.bmp', '*.gif']:\n",
        "        img_paths.extend(glob.glob(os.path.join(folder_path, ext)))\n",
        "    return img_paths\n",
        "\n",
        "# Load The images\n",
        "folder = '/content/drive/My Drive//'\n",
        "img_paths = load_images_from_folder(folder)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "# Function to generate captions for images\n",
        "def caption_images(input_image):\n",
        "    inputs = blip_processor(images=input_image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    pixel_values = inputs.pixel_values\n",
        "    generated_ids = blip_model.generate(pixel_values=pixel_values, max_length=50, num_beams=5, repetition_penalty=2.5)\n",
        "    generated_caption = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return generated_caption\n",
        "\n",
        "# Define paths\n",
        "metadata_file = os.path.join(goofy_folder, \"metadata.jsonl\")\n",
        "\n",
        "# Create metadata.jsonl file\n",
        "with open(metadata_file, 'w') as outfile:\n",
        "    for img_path in goofy_img_paths:\n",
        "        img_name = os.path.basename(img_path)\n",
        "        img = Image.open(img_path).convert('RGB').resize((256, 256), Image.LANCZOS)  # Use LANCZOS instead of ANTIALIAS\n",
        "        caption = caption_images(img)\n",
        "        entry = {\"file_name\": img_name, \"prompt\": f\"  , {caption}\"}\n",
        "        json.dump(entry, outfile, ensure_ascii=False)\n",
        "        outfile.write('\\n')"
      ],
      "metadata": {
        "id": "prYycCNwHj3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del blip_processor, blip_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "pgusoXQwJcRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!accelerate config default\n"
      ],
      "metadata": {
        "id": "GqvoRHYxJgwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login() #use hugging face secret key"
      ],
      "metadata": {
        "id": "WBAS9zOCJj8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch train_dreambooth_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
        "  --pretrained_vae_model_name_or_path=\"madebyollin/sdxl-vae-fp16-fix\" \\\n",
        "  --dataset_name=\"/content/drive/My Drive//\" \\  #adjust the path\n",
        "  --output_dir=\"/content/drive/My Drive/output/LoRA\" \\\n",
        "  --caption_column=\"prompt\" \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --instance_prompt=\"cartoon characters\" \\\n",
        "  --resolution=1024 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=3 \\\n",
        "  --gradient_checkpointing \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --snr_gamma=5.0 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --use_8bit_adam \\\n",
        "  --max_train_steps=500 \\\n",
        "  --checkpointing_steps=717 \\\n",
        "  --seed=\"0\""
      ],
      "metadata": {
        "id": "nboi1qa7Jk26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_folder\n",
        "\n",
        "# Replace with your actual Hugging Face username and repository name\n",
        "username = \"\"\n",
        "repository_name = \"\"\n",
        "\n",
        "# Path to your output directory on Google Drive same used in the fine tuning script\n",
        "output_dir = \"/content/drive/My Drive//\"\n",
        "\n",
        "# Upload to Hugging Face Hub\n",
        "upload_folder(\n",
        "    repo_id=f\"{username}/{repository_name}\",\n",
        "    folder_path=output_dir,\n",
        "    commit_message=\"Uploaded trained model\",\n",
        "    ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
        ")"
      ],
      "metadata": {
        "id": "z4Lly7X8BsfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "repo_id=\" \"\n",
        "link_to_model = f\"https://huggingface.co/{repo_id}\"\n",
        "display(Markdown(\"### Your model has finished training.\\nAccess it here: {}\".format(link_to_model)))\n"
      ],
      "metadata": {
        "id": "MkoSInlGB8Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    vae=vae,\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True\n",
        ")\n",
        "pipe.load_lora_weights(repo_id)\n",
        "_ = pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "1SFSgtT_CBAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metadata = {\n",
        "    \"characters\": {},\n",
        "    \"scenes\": {}\n",
        "}\n",
        "\n",
        "def generate_image_with_metadata(prompt, character_name=None, scene_name=None, metadata=None):\n",
        "    if character_name in metadata[\"characters\"]:\n",
        "        char_metadata = metadata[\"characters\"][character_name]\n",
        "        seed = char_metadata[\"seed\"]\n",
        "    else:\n",
        "        seed = torch.randint(0, 2**32, (1,)).item()\n",
        "        metadata[\"characters\"][character_name] = {\"seed\": seed}\n",
        "\n",
        "    if scene_name in metadata[\"scenes\"]:\n",
        "        scene_metadata = metadata[\"scenes\"][scene_name]\n",
        "        seed = scene_metadata[\"seed\"]\n",
        "    else:\n",
        "        seed = torch.randint(0, 2**32, (1,)).item()\n",
        "        metadata[\"scenes\"][scene_name] = {\"seed\": seed}\n",
        "\n",
        "    generator = torch.Generator(device=device).manual_seed(seed)\n",
        "    image = pipe(prompt, generator=generator).images[0]\n",
        "\n",
        "    return image\n",
        "\n",
        "# Define prompts and corresponding scene names\n",
        "prompts_and_scenes = [\n",
        "    (\" \", \" \"),\n",
        "    (\" \", \" \"),\n",
        "    (\" \", \" \"),\n",
        "    (\" \", \" \"),\n",
        "]\n",
        "# Generate and save multiple images with metadata\n",
        "generated_images = []\n",
        "for i, (prompt, scene_name) in enumerate(prompts_and_scenes):\n",
        "    if \" \" in prompt: #write character name\n",
        "        character_name = \" \"\n",
        "    else:\n",
        "        character_name = None\n",
        "\n",
        "    image = generate_image_with_metadata(prompt, character_name=character_name, scene_name=scene_name, metadata=metadata)\n",
        "    image.save(f\"generated_image_{i}.png\")\n",
        "    generated_images.append(image)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def display_images(images, titles):\n",
        "    if len(images) == 1:\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "        ax.imshow(images[0])\n",
        "        ax.set_title(titles[0])\n",
        "        ax.axis('off')\n",
        "    else:\n",
        "        fig, axes = plt.subplots(1, len(images), figsize=(20, 5))\n",
        "        for ax, img, title in zip(axes, images, titles):\n",
        "            ax.imshow(img)\n",
        "            ax.set_title(title)\n",
        "            ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display generated images\n",
        "titles = [scene_name for _, scene_name in prompts_and_scenes]\n",
        "display_images(generated_images, titles)\n"
      ],
      "metadata": {
        "id": "Tq1-AVLrCHg3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}